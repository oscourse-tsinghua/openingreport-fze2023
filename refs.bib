@techreport{treeshaking,
  author      = {杨健},
  institution = {浙江大学},
  title       = {Deep Dive into Rspack & Webpack Tree Shaking},
  year        = {2024},
  url         = {https://github.com/orgs/web-infra-dev/discussions/17}
}

@article{codereuse,
  title    = {CODE reuse in practice: Benefiting or harming technical debt},
  journal  = {Journal of Systems and Software},
  volume   = {167},
  pages    = {110618},
  year     = {2020},
  issn     = {0164-1212},
  doi      = {https://doi.org/10.1016/j.jss.2020.110618},
  url      = {https://www.sciencedirect.com/science/article/pii/S0164121220300960},
  author   = {Daniel Feitosa and Apostolos Ampatzoglou and Antonios Gkortzis and Stamatia Bibi and Alexander Chatzigeorgiou},
  keywords = {Technical debt, Reuse, Case study},
  abstract = {During the last years the TD community is striving to offer methods and tools for reducing the amount of TD, but also understand the underlying concepts. One popular practice that still has not been investigated in the context of TD, is software reuse. The aim of this paper is to investigate the relation between white-box code reuse and TD principal and interest. In particular, we target at unveiling if the reuse of code can lead to software with better levels of TD. To achieve this goal, we performed a case study on approximately 400 OSS systems, comprised of 897 thousand classes, and compare the levels of TD for reused and natively-written classes. The results of the study suggest that reused code usually has less TD interest; however, the amount of principal in them is higher. A synthesized view of the aforementioned results suggest that software engineers shall opt to reuse code when necessary, since apart from the established reuse benefits (i.e., cost savings, increased productivity, etc.) are also getting benefits in terms of maintenance. Apart from understanding the phenomenon per se, the results of this study provide various implications to research and practice.}
}

@article{compilevsinterpret,
  title   = {Qualitative Assessment of Compiled, Interpreted and Hybrid Programming Languages},
  journal = {Communications on Applied Electronics},
  volume  = {7},
  pages   = {8-13},
  year    = {2017},
  issn    = {2394-4714},
  doi     = {10.5120/cae2017652685},
  author  = {Ernest Kwame Ampomah and Ezekiel Mensah and Abilimi Gilbert}
}

@article{cg_rtl,
  title   = {基于RTL的函数调用图生成工具CG-RTL},
  author  = {孙卫真 and 杜香燕 and 向勇 and 汤卫东 and 侯鸿儒},
  journal = {小型微型计算机系统},
  volume  = {35},
  number  = {3},
  pages   = {5},
  year    = {2014}
}


@article{dcg_rtl,
  author       = {向勇 and 汤卫东 and 杜香燕 and 孙卫真},
  title        = { 基于内核跟踪的动态函数调用图生成方法 },
  organization = {清华大学 and 首都师范大学},
  journal      = {计算机应用研究},
  year         = {2015},
  volume       = {32},
  number       = {4},
  pages        = {1095-1099},
  month        = {1}
}

@article{dbcg_rtl,
  author       = {贾荻 and 向勇 and 孙卫真 and 曹睿东},
  title        = { 基于数据库的在线函数调用图工具 },
  organization = {首都师范大学 and 清华大学},
  journal      = {小型微型计算机系统},
  year         = {2016},
  volume       = {37},
  number       = {3},
  pages        = {422-427},
  month        = {1}
}

@article{Prazi,
  author     = {Hejderup, Joseph and Beller, Moritz and Triantafyllou, Konstantinos and Gousios, Georgios},
  title      = {Pr\"{a}zi: from package-based to call-based dependency networks},
  year       = {2022},
  issue_date = {Sep 2022},
  publisher  = {Kluwer Academic Publishers},
  address    = {USA},
  volume     = {27},
  number     = {5},
  issn       = {1382-3256},
  url        = {https://doi.org/10.1007/s10664-021-10071-9},
  doi        = {10.1007/s10664-021-10071-9},
  abstract   = {Modern programming languages such as Java, JavaScript, and Rust encourage software reuse by hosting diverse and fast-growing repositories of highly interdependent packages (i.e., reusable libraries) for their users. The standard way to study the interdependence between software packages is to infer a package dependency network by parsing manifest data. Such networks help answer questions such as “How many packages have dependencies to packages with known security issues?” or “What are the most used packages?”. However, an overlooked aspect in existing studies is that manifest-inferred relationships do not necessarily examine the actual usage of these dependencies in source code. To better model dependencies between packages, we developed Pr\"{a}zi, an approach combining manifests and call graphs of packages. Pr\"{a}zi constructs a dependency network at the more fine-grained function-level, instead of at the manifest level. This paper discusses a prototypical Pr\"{a}zi implementation for the popular system programming language Rust. We use Pr\"{a}zi to characterize Rust’s package repository, Crates.io, at the function level and perform a comparative study with metadata-based networks. Our results show that metadata-based networks generalize how packages use their dependencies. Using Pr\"{a}zi, we find packages call only 40\% of their resolved dependencies, and that manual analysis of 34 cases reveals that not all packages use a dependency the same way. We argue that researchers and practitioners interested in understanding how developers or programs use dependencies should account for its context—not the sum of all resolved dependencies.},
  journal    = {Empirical Softw. Engg.},
  month      = {sep},
  numpages   = {42},
  keywords   = {Package repository, Dependency network, Package manager, Software ecosystem, Network analysis, Call graphs}
}

@inproceedings{MirChecker,
  author    = {Li, Zhuohua and Wang, Jincheng and Sun, Mingshen and Lui, John C.S.},
  title     = {MirChecker: Detecting Bugs in Rust Programs via Static Analysis},
  year      = {2021},
  isbn      = {9781450384544},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3460120.3484541},
  doi       = {10.1145/3460120.3484541},
  abstract  = {Safe system programming is often a crucial requirement due to its critical role in system software engineering. Conventional low-level programming languages such as C and assembly are efficient, but their inherent unsafe nature makes it undesirable for security-critical scenarios. Recently, Rust has become a promising alternative for safe system-level programming. While giving programmers fine-grained hardware control, its strong type system enforces many security properties including memory safety. However, Rust's security guarantee is not a silver bullet. Runtime crashes and memory-safety errors still harass Rust developers, causing damaging exploitable vulnerabilities, as reported by numerous studies.In this paper, we present and evaluate MirChecker, a fully automated bug detection framework for Rust programs by performing static analysis on Rust's Mid-level Intermediate Representation (MIR). Based on the observation of existing bugs found in Rust codebases, our approach keeps track of both numerical and symbolic information, detects potential runtime crashes and memory-safety errors by using constraint solving techniques, and outputs informative diagnostics to users. We evaluate MirChecker on both buggy code snippets extracted from existing Common Vulnerabilities and Exposures (CVE) and real-world Rust codebases. Our experiments show that MirChecker can detect all the issues in our code snippets, and is capable of performing bug finding in real-world scenarios, where it detected a total of 33 previously unknown bugs including 16 memory-safety issues from 12 Rust packages (crates) with an acceptable false-positive rate.},
  booktitle = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
  pages     = {2183–2196},
  numpages  = {14},
  keywords  = {static analysis, rust, abstract interpretation},
  location  = {Virtual Event, Republic of Korea},
  series    = {CCS '21}
}

@inproceedings{Kani,
  author    = {VanHattum, Alexa and Schwartz-Narbonne, Daniel and Chong, Nathan and Sampson, Adrian},
  title     = {Verifying dynamic trait objects in rust},
  year      = {2022},
  isbn      = {9781450392266},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3510457.3513031},
  doi       = {10.1145/3510457.3513031},
  abstract  = {Rust has risen in prominence as a systems programming language in large part due to its focus on reliability. The language's advanced type system and borrow checker eliminate certain classes of memory safety violations. But for critical pieces of code, teams need assurance beyond what the type checker alone can provide. Verification tools for Rust can check other properties, from memory faults in unsafe Rust code to user-defined correctness assertions. This paper particularly focuses on the challenges in reasoning about Rust's dynamic trait objects, a feature that provides dynamic dispatch for function abstractions. While the explicit dyn keyword that denotes dynamic dispatch is used in 37\% of the 500 most-downloaded Rust libraries (crates), dynamic dispatch is implicitly linked into 70\%. To our knowledge, our open-source Kani Rust Verifier is the first symbolic modeling checking tool for Rust that can verify correctness while supporting the breadth of dynamic trait objects, including dynamically dispatched closures. We show how our system uses semantic trait information from Rust's Mid-level Intermediate Representation (an advantage over targeting a language-agnostic level such as LLVM) to improve verification performance by 5\%--15\texttimes{} for examples from open-source virtualization software. Finally, we share an open-source suite of verification test cases for dynamic trait objects.},
  booktitle = {Proceedings of the 44th International Conference on Software Engineering: Software Engineering in Practice},
  pages     = {321–330},
  numpages  = {10},
  keywords  = {dynamic dispatch, model checking, rust, verification},
  location  = {Pittsburgh, Pennsylvania},
  series    = {ICSE-SEIP '22}
}

@book{BrownBook,
  author    = {Brown, William H. and Malveau, Raphael C. and McCormick, Hays W. "Skip" and Mowbray, Thomas J.},
  title     = {AntiPatterns: Refactoring Software, Architectures, and Projects in Crisis},
  year      = {1998},
  isbn      = {0471197130},
  publisher = {John Wiley \& Sons, Inc.},
  address   = {USA},
  edition   = {1st},
  abstract  = {If patterns are good ideas that can be re-applied to new situations, AntiPatterns: Refactoring Software, Architectures, and Projects in Crisis looks at what goes wrong in software development, time and time again. This entertaining and often enlightening text defines what seasoned developers have long suspected: despite advances in software engineering, most software projects still fail to meet expectations--and about a third are cancelled altogether. The authors of AntiPatterns draw on extensive industry experience, their own and others, to help define what's wrong with software development today. They outline reasons why problem patterns develop (such as sloth, avarice, and greed) and proceed to outline several dozen patterns that can give you headaches or worse. Their deadliest hit list begins with the Blob, where one object does most of the work in a project, and Continuous Obsolescence, where technology changes so quickly that developers can't keep up. Some of the more entertaining antipatterns include the Poltergeist (where do-nothing classes add unnecessary overhead), the Boat Anchor (a white elephant piece of hardware or software bought at great cost) and the Golden Hammer (a single technology that is used for every conceivable programming problem). The authors then proceed to define antipatterns oriented toward management problems with software (including Death by Planning and Project Mismanagement, along with several miniature antipatterns, that help define why so many software projects are late and overbudget). The authors use several big vendors' technologies as examples of today's antipatterns. Luckily, they suggest ways to overcome antipatterns and improve software productivity in "refactored solutions" that can overcome some of these obstacles. However, this is a realistic book, a mix of "Dilbert" and software engineering. A clever antidote to getting too optimistic about software development, AntiPatterns should be required reading for any manager facing a large-scale development project. --Richard Dragan}
}

@inproceedings{Mantyla2003,
  author    = {Mantyla, M. and Vanhanen, J. and Lassenius, C.},
  booktitle = {International Conference on Software Maintenance, 2003. ICSM 2003. Proceedings.},
  title     = {A taxonomy and an initial empirical study of bad smells in code},
  year      = {2003},
  volume    = {},
  number    = {},
  pages     = {381-384},
  keywords  = {Taxonomy;Software maintenance;Software systems;Software quality;Programming;Internet;Software measurement;Cloning;Visualization;Logic},
  doi       = {10.1109/ICSM.2003.1235447}
}

@book{WakeBook,
  author    = {Wake, William C.},
  title     = {Refactoring Workbook},
  year      = {2003},
  isbn      = {0321109295},
  publisher = {Addison-Wesley Longman Publishing Co., Inc.},
  address   = {USA},
  edition   = {1},
  abstract  = {From the Publisher: As a programmer, you need to be able to recognize and improve problematic code, so the program remains in a working state throughout the software lifecycle. Refactoring-the art of improving the design of existing code safely-provides an efficient, reliable system for bringing order to the chaos, and one that keeps the surprises to a minimum! Refactoring can be difficult to learn, but Refactoring Workbook, by consultant William C. Wake, presents the material in a easy-learning format that makes learning enjoyable and effective. For many, the obstacle to learning refactoring is in identifying the "smells"the potential problem areas-found in code. Instead of having you read about the smells, Refactoring Workbook makes sure you understand them. You'll solve a carefully assembled series of problems, and you'll find yourself learning at a deeper level and arriving at a few insights of your own. Wake uses the workbook methoda learning-focused approach that forces you to apply the techniques presented in the book-in the rest of the book. This approach helps you learn and apply the most important refactoring techniques to your code and, as a side benefit, helps you to think more about creating great code even when you're not refactoring. Refactoring Workbook provides user-friendly references such as: A handy, quick-reference "smell finder" A standard format for describing smells Appendices showing key refactorings A listing of Java™ tools that support refactoring This book is intended for programmers with a knowledge of Java, though a C# or C++ programmer with a basic understanding of Javwould also be able to follow and learn from the examples. It can be used as a companion to Martin Fowler's Refactoring (also from Addison-Wesley Professional), which provides step-by-step instructions for many refactorings.}
}

@book{MartinBook,
  author    = {Martin, Robert C.},
  title     = {Clean Code: A Handbook of Agile Software Craftsmanship},
  year      = {2008},
  isbn      = {0132350882},
  publisher = {Prentice Hall PTR},
  address   = {USA},
  edition   = {1},
  abstract  = {Even bad code can function. But if code isnt clean, it can bring a development organization to its knees. Every year, countless hours and significant resources are lost because of poorly written code. But it doesnt have to be that way.Noted software expert Robert C. Martin, presents a revolutionary paradigm with Clean Code: A Handbook of Agile Software Craftsmanship. Martin, who has helped bring agile principles from a practitioners point of view to tens of thousands of programmers, has teamed up with his colleagues from Object Mentor to distill their best agile practice of cleaning code on the fly into a book that will instill within you the values of software craftsman, and make you a better programmerbut only if you work at it.What kind of work will you be doing? Youll be reading codelots of code. And you will be challenged to think about whats right about that code, and whats wrong with it. More importantly you will be challenged to reassess your professional values and your commitment to your craft. Clean Code is divided into three parts. The first describes the principles, patterns, and practices of writing clean code. The second part consists of several case studies of increasing complexity. Each case study is an exercise in cleaning up codeof transforming a code base that has some problems into one that is sound and efficient. The third part is the payoff: a single chapter containing a list of heuristics and smells gathered while creating the case studies. The result is a knowledge base that describes the way we think when we write, read, and clean code.Readers will come away from this book understandingHow to tell the difference between good and bad codeHow to write good code and how to transform bad code into good codeHow to create good names, good functions, good objects, and good classesHow to format code for maximum readability How to implement complete error handling without obscuring code logicHow to unit test and practice test-driven developmentWhat smells and heuristics can help you identify bad codeThis book is a must for any developer, software engineer, project manager, team lead, or systems analyst with an interest in producing better code.}
}

@article{CodeCompaction,
  author     = {Debray, Saumya K. and Evans, William and Muth, Robert and De Sutter, Bjorn},
  title      = {Compiler techniques for code compaction},
  year       = {2000},
  issue_date = {March 2000},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {22},
  number     = {2},
  issn       = {0164-0925},
  url        = {https://doi.org/10.1145/349214.349233},
  doi        = {10.1145/349214.349233},
  abstract   = {In recent years there has been an increasing trend toward the incorpor ation of computers into a variety of devices where the amount of memory available is limited. This makes it desirable to try to reduce the size of applications where possible. This article explores the use of compiler techniques to accomplish code compaction to yield smaller executables. The main contribution of this article is to show that careful, aggressive, interprocedural optimization, together with procedural abstraction of repeated code fragments, can yield significantly better reductions in code size than previous approaches, which have generally focused on abstraction of repeated instruction sequences. We also show how “equivalent” code fragments can be detected and factored out using conventional compiler techniques, and without having to resort to purely linear treatments of code sequences as in suffix-tree-based approaches, thereby setting up a framework for code compaction that can be more flexible in its treatment of what code fragments are considered equivalent. Our ideas have been implemented in the form of a binary-rewriting tool that reduces the size of executables by about 30\% on the average.},
  journal    = {ACM Trans. Program. Lang. Syst.},
  month      = {mar},
  pages      = {378–415},
  numpages   = {38},
  keywords   = {code compaction, code compression, code size reduction}
}

@inproceedings{BetterUnderstanding,
  author    = {Godfrey and Qiang Tu},
  booktitle = {Proceedings 2000 International Conference on Software Maintenance},
  title     = {Evolution in open source software: a case study},
  year      = {2000},
  volume    = {},
  number    = {},
  pages     = {131-142},
  keywords  = {Software prototyping},
  doi       = {10.1109/ICSM.2000.883030}
}

@inproceedings{MetricsUnderstanding,
  author    = {Oman, P. and Hagemeister, J.},
  booktitle = {Proceedings Conference on Software Maintenance 1992},
  title     = {Metrics for assessing a software system's maintainability},
  year      = {1992},
  volume    = {},
  number    = {},
  pages     = {337-344},
  keywords  = {Software maintenance;Software systems;Software measurement;Tree data structures;Documentation;Taxonomy;Environmental management;Lifting equipment;Software engineering;Software testing},
  doi       = {10.1109/ICSM.1992.242525}
}

@inproceedings{SoftwareAgeing,
  author    = {Parnas, David Lorge},
  title     = {Software aging},
  year      = {1994},
  isbn      = {081865855X},
  publisher = {IEEE Computer Society Press},
  address   = {Washington, DC, USA},
  booktitle = {Proceedings of the 16th International Conference on Software Engineering},
  pages     = {279–287},
  numpages  = {9},
  location  = {Sorrento, Italy},
  series    = {ICSE '94}
}


@article{MultiStudy,
  author   = {Romano, Simone and Vendome, Christopher and Scanniello, Giuseppe and Poshyvanyk, Denys},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {A Multi-Study Investigation into Dead Code},
  year     = {2020},
  volume   = {46},
  number   = {1},
  pages    = {71-99},
  keywords = {Software systems;Maintenance engineering;Software engineering;Interviews;Tools;Open source software;Dead code;unreachable code;unused code;bad smell;empirical investigation;multi-study},
  doi      = {10.1109/TSE.2018.2842781}
}

@inproceedings{PHPWebSystem,
  author    = {Boomsma, Hidde and Hostnet, B. V. and Gross, Hans-Gerhard},
  booktitle = {2012 28th IEEE International Conference on Software Maintenance (ICSM)},
  title     = {Dead code elimination for web systems written in PHP: Lessons learned from an industry case},
  year      = {2012},
  volume    = {},
  number    = {},
  pages     = {511-515},
  keywords  = {Software maintenance;Maintenance engineering;Conferences;Image color analysis;Runtime},
  doi       = {10.1109/ICSM.2012.6405314}
}

@article{DynAnalysis,
  author     = {Ball, Thoms},
  title      = {The concept of dynamic analysis},
  year       = {1999},
  issue_date = {Nov. 1999},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {24},
  number     = {6},
  issn       = {0163-5948},
  url        = {https://doi.org/10.1145/318774.318944},
  doi        = {10.1145/318774.318944},
  abstract   = {Dynamic analysis is the analysis of the properties of a running program. In this paper, we explore two new dynamic analyses based on program profiling:Frequency Spectrum Analysis. We show how analyzing the frequencies of program entities in a single execution can help programmers to decompose a program, identify related computations, and find computations related to specific input and output characteristics of a program.Coverage Concept Analysis. Concept analysis of test coverage data computes dynamic analogs to static control flow relationships such as domination, postdomination, and regions. Comparison of these dynamically computed relationships to their static counterparts can point to areas of code requiring more testing and can aid programmers in understanding how a program and its test sets relate to one another.},
  journal    = {SIGSOFT Softw. Eng. Notes},
  month      = {oct},
  pages      = {216–234},
  numpages   = {19}
}

@inproceedings{DUM,
  author    = {Romano, Simone and Scanniello, Giuseppe and Sartiani, Carlo and Risi, Michele},
  title     = {A graph-based approach to detect unreachable methods in Java software},
  year      = {2016},
  isbn      = {9781450337397},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2851613.2851968},
  doi       = {10.1145/2851613.2851968},
  abstract  = {In this paper, we have defined a static approach named DUM (Detecting Unreachable Methods) that works on Java byte-code and detects unreachable methods by traversing a graph-based representation of the software to be analyzed. To assess the validity of our approach, we have implemented it in a prototype software system. Both our approach and prototype have been validated on four open-source software. Results have shown the correctness, the completeness, and the accuracy of the methods that our solution detected as unreachable. We have also compared our solution with: JTombstone and Google CodePro AnalytiX. This comparison suggested that DUM outperforms baselines.},
  booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
  pages     = {1538–1541},
  numpages  = {4},
  keywords  = {bad smells, software maintenance, unreachable methods},
  location  = {Pisa, Italy},
  series    = {SAC '16}
}

@article{GraphBuilding,
  author     = {Tip, Frank and Palsberg, Jens},
  title      = {Scalable propagation-based call graph construction algorithms},
  year       = {2000},
  issue_date = {Oct. 2000},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {35},
  number     = {10},
  issn       = {0362-1340},
  url        = {https://doi.org/10.1145/354222.353190},
  doi        = {10.1145/354222.353190},
  abstract   = {Propagation-based call graph construction algorithms have been studied intensively in the 199Os, and differ primarily in the number of sets that are used to approximate run-time values of expressions. In practice, algorithms such as RTA that use a single set for the whole program scale well. The scalability of algorithms such as 0-CFA that use one set per expression remains doubtful.In this paper, we investigate the design space between RTA and 0-CFA. We have implemented various novel algorithms in the context of Jax, an application extractor for Java, and shown that they all scale to a 325,000-line program. A key property of these algorithms is that they do not analyze values on the run-time stack, which makes them efficient and easy to implement. Surprisingly, for detecting unreachable methods, the inexpensive RTA algorithm does almost as well as the seemingly more powerful algorithms. However, for determining call sites with a single target, one of our new algorithms obtains the current best tradeoff between speed and precision.},
  journal    = {SIGPLAN Not.},
  month      = {oct},
  pages      = {281–293},
  numpages   = {13}
}

@inproceedings{Rupta,
  author    = {Li, Wei and He, Dongjie and Gui, Yujiang and Chen, Wenguang and Xue, Jingling},
  title     = {A Context-Sensitive Pointer Analysis Framework for Rust and Its Application to Call Graph Construction},
  year      = {2024},
  isbn      = {9798400705076},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3640537.3641574},
  doi       = {10.1145/3640537.3641574},
  abstract  = {Existing program analysis tools for Rust lack the ability to effectively detect security vulnerabilities due to the absence of an accurate call graph and precise points-to information. We present Rupta, the first context-sensitive pointer analysis framework designed for Rust, with a particular focus on its role in constructing call graphs. Operating on Rust MIR, Rupta employs callsite-based context-sensitivity and on-the-fly call graph construction to address a range of pointer analysis challenges, including method/function calls, pointer casts, and nested structs, while preserving type information.
               
               Our assessment of Rupta against two state-of-the-art call graph construction techniques, Rurta (Rapid Type Analysis-based) and Ruscg (static dispatch-only), across 13 real-world Rust programs demonstrates its high efficiency and precision. In particular, our results reveal that Rupta surpasses Ruscg in soundness by discovering 29\% more call graph edges and outperforms Rurta in precision by eliminating approximately 70\% of spurious dynamic call edges. Consequently, Rupta has the potential to enhance existing security analysis tools, enabling them to identify a greater number of security vulnerabilities in Rust programs.},
  booktitle = {Proceedings of the 33rd ACM SIGPLAN International Conference on Compiler Construction},
  pages     = {60–72},
  numpages  = {13},
  keywords  = {Call Graph Construction, Pointer Analysis, Rust},
  location  = {<conf-loc>, <city>Edinburgh</city>, <country>United Kingdom</country>, </conf-loc>},
  series    = {CC 2024}
}

@inproceedings{494744,
  author    = {Tanaka, T. and Hakuta, M. and Iwata, N. and Ohminami, M.},
  booktitle = {Proceedings of the 12th TRON Project International Symposium},
  title     = {Approaches to making software porting more productive},
  year      = {1995},
  volume    = {},
  number    = {},
  pages     = {73-85},
  keywords  = {Productivity;Impedance;Communication system software;Databases;Communication system control;Laboratories;Operating systems;Testing;Software libraries;Electronics packaging},
  doi       = {10.1109/TRON.1995.494744}
}

@article{HAKUTA1997145,
  title    = {A study of software portability evaluation},
  journal  = {Journal of Systems and Software},
  volume   = {38},
  number   = {2},
  pages    = {145-154},
  year     = {1997},
  issn     = {0164-1212},
  doi      = {https://doi.org/10.1016/S0164-1212(96)00118-5},
  url      = {https://www.sciencedirect.com/science/article/pii/S0164121296001185},
  author   = {Mitsuari Hakuta and Masato Ohminami},
  abstract = {The authors propose a model for estimating the cost of software porting, based on program characteristics and other factors (porting impediment factors). We first clarify the porting impediment factors and define evaluation criteria for each factor. A model for evaluating porting cost is derived from these factors. The applicability of the model framework is confirmed by porting various programs. Measures for reducing porting cost are proposed, and their effects are estimated using the model. The proposed model can be used to estimate porting cost and to evaluate porting productivity as well as the productivity of each stage of the porting process.}
}

@article{PortingAndLoC,
  author     = {Wolberg, John R.},
  title      = {Comparing the cost of software conversion to the cost of reprogramming},
  year       = {1981},
  issue_date = {April 1981},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {16},
  number     = {4},
  issn       = {0362-1340},
  url        = {https://doi.org/10.1145/988131.988144},
  doi        = {10.1145/988131.988144},
  journal    = {SIGPLAN Not.},
  month      = {apr},
  pages      = {104–110},
  numpages   = {7}
}

@inproceedings{NewerPortingReview,
  author    = {Ghandorh, Hamza and Noorwali, Abdulfattah and Nassif, Ali Bou and Capretz, Luiz Fernando and Eagleson, Roy},
  title     = {A Systematic Literature Review for Software Portability Measurement: Preliminary Results},
  year      = {2020},
  isbn      = {9781450376655},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3384544.3384569},
  doi       = {10.1145/3384544.3384569},
  abstract  = {Software developers agree that software portability is a desirable attribute for their software quality. Software portability is mostly acquired by ad-hoc techniques when trying to port existing products. There is a lack of unified measuring approach of software portability in most computing platforms. This paper presents preliminary results of a systematic literature review, conducted to collect evidence on measuring software portability. The evidence was gathered from selected studies and based on a set of meaningful and focused questions. 49 studies of these were selected for data extraction performed against the research questions. We provide an overview of usedproposed measurement metrics of software portability. Our results suggested that there are scattered efforts to understand measurement of software portability, and no census has been achieved.},
  booktitle = {Proceedings of the 2020 9th International Conference on Software and Computer Applications},
  pages     = {152–157},
  numpages  = {6},
  keywords  = {Empirical study, Software measurement, Software portability, Software quality},
  location  = {Langkawi, Malaysia},
  series    = {ICSCA '20}
}

@inproceedings{DesirablePortability,
  author    = {Mooney, James D.},
  editor    = {Reis, Ricardo},
  title     = {Developing Portable Software},
  booktitle = {Information Technology},
  year      = {2004},
  publisher = {Springer US},
  address   = {Boston, MA},
  pages     = {55--84},
  abstract  = {Software portability is often cited as desirable, but rarely receives systematic attention in the software development process. With the growing diversity of computing platforms, it is increasingly likely that software of all types may need to migrate to a variety of environments and platforms over its lifetime. This tutorial is intended to show the reader how to design portability into software projects, and how to port software when required.},
  isbn      = {978-1-4020-8159-0}
}

@inproceedings{ai_and_code_errors,
  author    = {Brun, Y. and Ernst, M.D.},
  booktitle = {Proceedings. 26th International Conference on Software Engineering},
  title     = {Finding latent code errors via machine learning over program executions},
  year      = {2004},
  volume    = {},
  number    = {},
  pages     = {480-490},
  keywords  = {Machine learning;Testing;Programming profession;Computer errors;Support vector machines;Support vector machine classification;Decision trees;Classification tree analysis;Laboratories;Computer science},
  doi       = {10.1109/ICSE.2004.1317470}
}

@inproceedings{through_lens_of_dead_code_elimination,
  author    = {Theodoridis, Theodoros and Rigger, Manuel and Su, Zhendong},
  title     = {Finding missed optimizations through the lens of dead code elimination},
  year      = {2022},
  isbn      = {9781450392051},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3503222.3507764},
  doi       = {10.1145/3503222.3507764},
  booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages     = {697–709},
  numpages  = {13},
  keywords  = {compilers, missed optimizations, testing},
  location  = {Lausanne, Switzerland},
  series    = {ASPLOS '22}
}

@inproceedings{unused_code_matters,
  author    = {Eder, Sebastian and Junker, Maximilian and Jürgens, Elmar and Hauptmann, Benedikt and Vaas, Rudolf and Prommer, Karl-Heinz},
  booktitle = {2012 34th International Conference on Software Engineering (ICSE)},
  title     = {How much does unused code matter for maintenance?},
  year      = {2012},
  volume    = {},
  number    = {},
  pages     = {1102-1111},
  keywords  = {Maintenance engineering;Assembly;Software systems;Business;Information systems;Production;Software maintenance;dynamic analysis;unnecessary code;unused code},
  doi       = {10.1109/ICSE.2012.6227109}
}

@article{Ray2012ACS,
  title   = {A case study of cross-system porting in forked projects},
  author  = {Baishakhi Ray and Miryung Kim},
  journal = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering},
  year    = {2012},
  volume  = {},
  pages   = {}
}

@article{Li2023PortingAE,
  title   = {Porting and Enhancing the XV6 Kernel for LoongArch ArchitectureElevating Functionality: A Unified Approach to Kernel Porting and Enhancement},
  author  = {Qi Li and Wei Xu and Siyue Chen and Ziwen Pang},
  journal = {},
  year    = {2023},
  volume  = {},
  pages   = {1-5}
}